{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yeast cells detection evaluation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "#sys.path.append('%HOMEPATH%/detectron2') #insert local path to detecron2\n",
    "import numpy as np\n",
    "\n",
    "import yeastcells.clustering as clustering\n",
    "import yeastcells.features as features\n",
    "import yeastcells.data as data\n",
    "import yeastcells.model as model\n",
    "import yeastcells.visualize as visualize\n",
    "import yeastcells.evaluation as evaluation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS = 'TestSet7' #choose YIT test set\n",
    "path = f'../../slow-data/yeastcells/data/cellstar_benchmark/{TS}'\n",
    "filenames = data.load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.read_images_cat(filenames)\n",
    "np.shape(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and run model on data for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_path = f'{path}/GroundTruth_Segmentation.csv'\n",
    "track_path = f'{path}/GroundTruth_Tracking.csv'\n",
    "gt_s, gt_t = data.get_gt_yit(seg_path, track_path)\n",
    "\n",
    "#set path to model_final.pth\n",
    "model_path = '/var/tensorflow-logs/p253591/yeast-cell-detection-run-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_s = []\n",
    "recall_s = []\n",
    "accuracy_s = []\n",
    "F_s = []\n",
    "precision_t = []\n",
    "recall_t = []\n",
    "accuracy_t = []\n",
    "F_t = []\n",
    "\n",
    "sts = [0.8,.81,.82,.83,.84,.85,.86,.87,.88,.89,.9,.91,.92,.93,.94,.95,.96,.97,.98,.99]\n",
    "\n",
    "for i in range(len(sts)):\n",
    "#load model\n",
    "    predictor = model.load_model(model_path, seg_thresh=sts[i], device='cuda:0') #set GPU if available otherwise use 'cpu'\n",
    "\n",
    "    output = [\n",
    "      {'instances': predictor(frame)['instances'].to('cpu')}\n",
    "      for frame in image\n",
    "    ]\n",
    "\n",
    "    labels, coordinates = clustering.cluster_cells(output, dmax=5, min_samples=3, eps=0.6, progress=False)\n",
    "\n",
    "    pred_s, pred_t, _ = data.get_pred(output, labels, coordinates)\n",
    "\n",
    "    pred_s_, results_s = evaluation.get_seg_performance(pred_s, gt_s, output, pipeline='maskrcnn') \n",
    "    pred_t_, results_t = evaluation.get_track_performance(pred_t, gt_t, output, pipeline='maskrcnn')\n",
    "\n",
    "    metrics_s = evaluation.calculate_metrics(results_s, pred_s_, gt_s)\n",
    "    metrics_t = evaluation.calculate_metrics(results_t, pred_t_, gt_t)\n",
    "    \n",
    "    precision_s.append(metrics_s['Precision'])\n",
    "    recall_s.append(metrics_s['Recall'])\n",
    "    accuracy_s.append(metrics_s['Accuracy'])\n",
    "    F_s.append(metrics_s['F1-score'])\n",
    "    precision_t.append(metrics_t['Precision'])\n",
    "    recall_t.append(metrics_t['Recall'])\n",
    "    accuracy_t.append(metrics_t['Accuracy'])\n",
    "    F_t.append(metrics_t['F1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f'Segmentation performance Mask RCNN {TS}')\n",
    "plt.xlabel('Segmentation threshold')\n",
    "plt.ylabel('Metrics')\n",
    "plt.plot(sts, F_s, label='F1-score')\n",
    "plt.plot(sts, accuracy_s, label='accuracy')\n",
    "plt.plot(sts, precision_s, label='precision')\n",
    "plt.plot(sts, recall_s, label='recall')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f'Tracking performance Mask RCNN {TS}')\n",
    "plt.xlabel('Segmentation threshold')\n",
    "plt.ylabel('Metrics')\n",
    "plt.plot(sts, F_t, label='F1-score')\n",
    "plt.plot(sts, accuracy_t, label='accuracy')\n",
    "plt.plot(sts, precision_t, label='precision')\n",
    "plt.plot(sts, recall_t, label='recall')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun pipeline with optimized sts\n",
    "predictor = model.load_model(model_path, seg_thresh=0.955, device='cuda:0') #set GPU if available otherwise use 'cpu'\n",
    "output = [\n",
    "      {'instances': predictor(frame)['instances'].to('cpu')}\n",
    "      for frame in image\n",
    "    ]\n",
    "labels, coordinates = clustering.cluster_cells(output, dmax=5, min_samples=3, eps=0.6, progress=False)\n",
    "pred_s, pred_t, _ = data.get_pred(output, labels, coordinates)\n",
    "pred_s_, results_s = evaluation.get_seg_performance(pred_s, gt_s, output, pipeline='maskrcnn') \n",
    "pred_t_, results_t = evaluation.get_track_performance(pred_t, gt_t, output, pipeline='maskrcnn')\n",
    "metrics_s = evaluation.calculate_metrics(results_s, pred_s_, gt_s)\n",
    "metrics_t = evaluation.calculate_metrics(results_t, pred_t_, gt_t) \n",
    "\n",
    "print(\"%.4f\" % metrics_s['Precision'])\n",
    "print(\"%.4f\" % metrics_s['Recall'])\n",
    "print(\"%.4f\" % metrics_s['Accuracy'])\n",
    "print(\"%.4f\" % metrics_s['F1-score'])\n",
    "print(\"%.4f\" % metrics_t['Precision'])\n",
    "print(\"%.4f\" % metrics_t['Recall'])\n",
    "print(\"%.4f\" % metrics_t['Accuracy'])\n",
    "print(\"%.4f\" % metrics_t['F1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yeast Cells",
   "language": "python",
   "name": "yeastcells"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
